{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import split_data_to_traj_and_control, mat2tracks\n",
    "import wandb\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from train_utils import train_epoch, eval_epoch\n",
    "from models import SplittedModel, SplittedModel2, DummyModel2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/10_traj'),\n",
       " WindowsPath('data/25_traj'),\n",
       " WindowsPath('data/50_traj'),\n",
       " WindowsPath('data/100_traj'),\n",
       " WindowsPath('data/200_traj'),\n",
       " WindowsPath('data/500_traj'),\n",
       " WindowsPath('data/750_traj'),\n",
       " WindowsPath('data/1000_traj')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = \"./data\"\n",
    "data_root = Path(\"./data\")\n",
    "folder_names = os.listdir(data_root)\n",
    "folder_names.sort(key = lambda x: int(x.split(\"_\")[0]))\n",
    "folder_paths = [data_root / i for i in folder_names]\n",
    "folder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir_path =  folder_paths[0]\n",
    "\n",
    "train_mat_path = cur_dir_path / \"sdreDataset.mat\"\n",
    "val_mat_path = cur_dir_path / \"sdreVal.mat\"\n",
    "\n",
    "train = scipy.io.loadmat(train_mat_path)[\"dataset\"]\n",
    "val = scipy.io.loadmat(val_mat_path)[\"sdreVal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010, 3, 3) (10100, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders_from_track(train, val, reshape=True):\n",
    "    train_tracks = mat2tracks(train, reshape=reshape)\n",
    "    val_tracks = mat2tracks(val, reshape=reshape)\n",
    "    train_tracks = np.vstack(train_tracks) \n",
    "    val_tracks = np.vstack(val_tracks)\n",
    "\n",
    "    train_dataset = split_data_to_traj_and_control(train_tracks)\n",
    "    test_dataset = split_data_to_traj_and_control(val_tracks)\n",
    "    print(f\"len(train) = {len(train_dataset)} len(test) = {len(test_dataset)}\")\n",
    "  \n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=64, \n",
    "                            shuffle=True,\n",
    "                            drop_last=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                            batch_size=64)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_crit_opt(hidden_dim_1=64, \n",
    "                       hidden_dim_2=64, \n",
    "                       dropout_rate=0., \n",
    "                       type_model=\"monolit\"):\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if type_model == \"monolit\":\n",
    "        model = DummyModel2(hidden_dim_1=hidden_dim_1, \n",
    "                        hidden_dim_2=hidden_dim_2, \n",
    "                        dropout_rate=dropout_rate)\n",
    "        \n",
    "    elif type_model == \"anfislike\":\n",
    "        model = SplittedModel2(hidden_dim_1=hidden_dim_1, \n",
    "                        hidden_dim_2=hidden_dim_2)\n",
    "    \n",
    "    model.to(device)\n",
    "    criteria = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    return model, device, criteria, optimizer\n",
    "\n",
    "def train_one_model(train, val, \n",
    "                    n_epoch=60,\n",
    "                    hidden_dim_1=64, \n",
    "                    hidden_dim_2=64, \n",
    "                    dropout_rate=0.,\n",
    "                    wandb_loggging=False, \n",
    "                    save_weights=False, \n",
    "                    type_model=\"monolit\"):\n",
    "\n",
    "    train_loader, test_loader = prepare_dataloaders_from_track(train, val)\n",
    "\n",
    "    model, device, criteria, optimizer = get_model_crit_opt(hidden_dim_1=hidden_dim_1,\n",
    "                                                             hidden_dim_2=hidden_dim_2,\n",
    "                                                               dropout_rate=dropout_rate, \n",
    "                                                               type_model=type_model)\n",
    "\n",
    "    best_loss = 1e6\n",
    "\n",
    "    if save_weights:\n",
    "        save_path = f\"MLP_3_{hidden_dim_1}_{hidden_dim_2}_6_best.pth\"\n",
    "        save_path = f\"splitted_model_best.pth\"\n",
    "\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        train_epoch(model, device, train_loader, criteria, optimizer)\n",
    "        val_loss = eval_epoch(model, device, test_loader, criteria)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            if save_weights:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            # print(f\"Improve eval losss on epoch {epoch} = \", best_loss)\n",
    "            \n",
    "        if wandb_loggging:\n",
    "            wandb.log({\n",
    "                \"val_loss\": val_loss,\n",
    "                \"epoch\" : epoch\n",
    "                })\n",
    "            \n",
    "    return best_loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monolit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) = 1010 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:16<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.017369501292705536\n",
      "len(train) = 2525 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:19<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.013264141045510769\n",
      "len(train) = 5050 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:23<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.007438871543854475\n",
      "len(train) = 10100 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:36<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.006379625294357538\n",
      "len(train) = 20200 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.00578113179653883\n",
      "len(train) = 50500 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:06<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.005083444528281689\n",
      "len(train) = 75750 len(test) = 10100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [02:48<02:54,  9.16s/it]"
     ]
    }
   ],
   "source": [
    "n_traj = []\n",
    "losses = []\n",
    "\n",
    "for cur_dir_path in folder_paths:\n",
    "    train_mat_path = cur_dir_path / \"sdreDataset.mat\"\n",
    "    val_mat_path = cur_dir_path / \"sdreVal.mat\"\n",
    "    train = scipy.io.loadmat(train_mat_path)[\"dataset\"]\n",
    "    val = scipy.io.loadmat(val_mat_path)[\"sdreVal\"]\n",
    "    best_loss = train_one_model(train, val, type_model=\"monolit\")\n",
    "    \n",
    "    losses.append(best_loss)\n",
    "    n_traj.append(int(str(cur_dir_path).split(\"\\\\\")[1].split(\"_\")[0]))\n",
    "    print(f\"best_loss = {best_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anfislike Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traj_anfis = []\n",
    "losses_anfis = []\n",
    "\n",
    "for cur_dir_path in folder_paths:\n",
    "    train_mat_path = cur_dir_path / \"sdreDataset.mat\"\n",
    "    val_mat_path = cur_dir_path / \"sdreVal.mat\"\n",
    "    train = scipy.io.loadmat(train_mat_path)[\"dataset\"]\n",
    "    val = scipy.io.loadmat(val_mat_path)[\"sdreVal\"]\n",
    "    best_loss = train_one_model(train, val, type_model=\"anfislike\")\n",
    "    \n",
    "    losses_anfis.append(best_loss)\n",
    "    n_traj_anfis.append(int(str(cur_dir_path).split(\"\\\\\")[1].split(\"_\")[0]))\n",
    "    print(f\"best_loss = {best_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(n_traj, losses)\n",
    "plt.plot(n_traj, losses, \"--\", label=\"monoolit\")\n",
    "plt.plot(n_traj_anfis, losses_anfis, \"--\", label=\"monoolit\", , label=\"anfislike\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Число треков для обучения NN\")\n",
    "plt.ylabel(\"MSE NN на валидации\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_data = {\"n_traj\" : n_traj, \n",
    "             \"losses\" : losses}\n",
    "\n",
    "savename = \"monolit_model\"\n",
    "\n",
    "with open(f\"{savename}.pickle\", 'wb') as handle:\n",
    "    pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_data = {\"n_traj\" : n_traj_anfis, \n",
    "             \"losses\" : losses_anfis}\n",
    "\n",
    "savename = \"3_model_anfislike\"\n",
    "\n",
    "with open(f\"{savename}.pickle\", 'wb') as handle:\n",
    "    pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_dir_path = folder_paths[0]\n",
    "\n",
    "# train_mat_path = cur_dir_path / \"sdreDataset.mat\"\n",
    "# val_mat_path = cur_dir_path / \"sdreVal.mat\"\n",
    "# train = scipy.io.loadmat(train_mat_path)[\"dataset\"]\n",
    "# val = scipy.io.loadmat(val_mat_path)[\"sdreVal\"]\n",
    "\n",
    "# best_loss = train_one_model(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cur_dir_path in folder_paths:\n",
    "#     train_mat_path = cur_dir_path / \"sdreDataset.mat\"\n",
    "#     val_mat_path = cur_dir_path / \"sdreVal.mat\"\n",
    "\n",
    "#     train = scipy.io.loadmat(train_mat_path)[\"dataset\"]\n",
    "#     val = scipy.io.loadmat(val_mat_path)[\"sdreVal\"]\n",
    "#     print(train.shape, val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
